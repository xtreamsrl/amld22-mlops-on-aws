{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense model\n",
    "It is end of May 2020. \n",
    "\n",
    "Your models worked. You managed to develop accurate short-term predictors in a matter of days.\n",
    "\n",
    "Now, you are asked to explain how you did it. And you decide to do it the hard way. You will show your colleagues a demo and explain it step by step.\n",
    "\n",
    "You enter the room, look at the audience, and recognize familiare faces: Gabriele and Matteo, the ML engineers, Marta and Gabriele, your fellow data scientists, and Paolo, the new guy of the group.\n",
    "\n",
    "As always, you start from the problem statement.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Problem Statement</b> \n",
    "    \n",
    "Given Italian daily power load data from 2006 to day $t$, predict the load on day $t+1$.\n",
    "</div>\n",
    "\n",
    "Your first idea was an autoregressive model. \n",
    "\n",
    "You wanted to be flexible, so you opted for a dense neural network. By no means it is the state of the art, but it was just a first shot.\n",
    "\n",
    "You start to explain how you trained the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Again, you update the default packages in SageMaker studio.\n",
    "\n",
    "Please, restart the kernel if this is the first time you run this notebook: it is necessary to ensure that we can actually import the libraries we've just installed in the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read data from S3\n",
    "! pip install pandas s3fs --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ml.m5.xlarge' is included in the AWS Free Tier\n",
    "INSTANCE_TYPE = 'ml.m5.xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the default size for matplotlib plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data gathering\n",
    "As in Fourier Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_bucket = sagemaker_session.default_bucket()\n",
    "main_prefix = \"amld22-workshop-sagemaker\"\n",
    "\n",
    "raw_data_s3_path = \"s3://public-workshop/normalized_data/processed/2006_2022_data.parquet\"\n",
    "raw_df = pd.read_parquet(raw_data_s3_path)\n",
    "resampled_df = raw_df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You consider two datasets:\n",
    "- data until the end of 2019: the same data that has been used to train the long-term models\n",
    "- data until the end of May 2020: to show the behaviour of models during the pandemic period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = resampled_df[:'2019-12-31 23:59'].copy()\n",
    "covid_df = resampled_df[:'2020-05-31 23:59'].copy()\n",
    "\n",
    "covid_len = covid_df.shape[0] - data_df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You use as **training** set the data until the end of 2019, and as **test** set the data until the end of May 2020.   \n",
    "To prepare the dataset for the neural network, you write a utility function to transform the load series into a dataframe of lagged features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To transform a series of datapoints into a dataframe that contains the lagged features\n",
    "def build_lagged_df(series: pd.Series, n_lags: int) -> pd.DataFrame:\n",
    "    df = pd.DataFrame({series.name: series})\n",
    "    for i in range(1, n_lags + 1):\n",
    "        df[f'{series.name}_{i}'] = series.shift(i)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lags = 7\n",
    "\n",
    "# We rescale the dataset using the Max value within the training set\n",
    "covid_max = data_df.Load.max()\n",
    "dense_df = build_lagged_df(covid_df.Load / covid_max, n_lags=n_lags)\n",
    "\n",
    "x_train_dense_scaled, x_test_dense_scaled, y_train_dense_scaled, y_test_dense_scaled = train_test_split(\n",
    "    dense_df.drop(columns=['Load']),\n",
    "    dense_df.Load,\n",
    "    test_size=covid_len,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train set length: {x_train_dense_scaled.shape[0]} | Test set length: {x_test_dense_scaled.shape[0]}\")\n",
    "\n",
    "train_df = dense_df.copy().loc[x_train_dense_scaled.index]\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the Fourier regression, you upload the data to S3.\n",
    "\n",
    "This is needed to train the TensorFlow estimator in framework mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_train_path = f's3://{sagemaker_bucket}/{main_prefix}/data/modelling/dense-model/train.parquet'\n",
    "train_df.to_parquet(s3_train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to minimize the friction in using TensorFlow, you choose a managed container and go for framework mode.\n",
    "\n",
    "In SageMaker framework mode, you specify the entry point `dense_model.py` as the Python script to use within the TensorFlow container.\n",
    "\n",
    "The script defines the neural net's structure and trains it on the training set according to the hyperparameters passed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_OF_EPOCHS = 300\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "dense_estimator = TensorFlow(\n",
    "    entry_point='dense_model.py',\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    framework_version=\"2.4.1\",\n",
    "    py_version=\"py37\",\n",
    "    hyperparameters={\n",
    "        \"num_of_epochs\": NUM_OF_EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"version_number\": \"0000001\"\n",
    "    }\n",
    ")\n",
    "dense_estimator.fit({'training': s3_train_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "Using the facility of AWS SageMaker, you deploy the model to a managed endpoint.\n",
    "\n",
    "On inference, the model trained in the `dense_model.py` module will be retrieved and asked to predict with new data.\n",
    "\n",
    "Unlike Fourier regression, you do not rely on the Feature Store, so there are no limitations on the predicted number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_predictor = dense_estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    serializer=sagemaker.serializers.CSVSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "Finally, you use the deployed model to perform some predictions. \n",
    "\n",
    "You use the estimator client, but your fellow engineers will be able to call the API using any REST client.\n",
    "\n",
    "As you selected the CSV serializer, you can pass a numpy array directly to the `.predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = dense_predictor.predict(x_test_dense_scaled.to_numpy())\n",
    "y_pred = pd.Series([y[0] for y in prediction['predictions']], index=x_test_dense_scaled.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred) / y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_mape = mean_absolute_percentage_error(y_test_dense_scaled * covid_max, y_pred * covid_max)\n",
    "\n",
    "plt.title(f\"Dense model | MAPE: {100 * dense_mape:.2f} %\")\n",
    "plt.plot(y_test_dense_scaled * covid_max, label='Actual')\n",
    "plt.plot(y_pred * covid_max, label='Predicted')\n",
    "plt.legend()\n",
    "plt.grid(0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_prediction_df = pd.DataFrame({'actual': y_test_dense_scaled * covid_max, 'predicted': y_pred * covid_max})\n",
    "rolling_mape_df = pd.DataFrame(\n",
    "    {'rolling_mape': map(lambda w: mean_absolute_percentage_error(w[\"actual\"], w[\"predicted\"]),\n",
    "                         covid_prediction_df.rolling(7, method=\"table\"))},\n",
    "    index=covid_prediction_df.index\n",
    ")\n",
    "plt.plot(rolling_mape_df.rolling_mape)\n",
    "plt.title('Rolling MAPE (7-day window)')\n",
    "plt.grid(0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rolling MAPE is way better than before. You realize that the scale is different: not the max is 0.1, using the Fourier regression it was 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Yes, it is a completely different beast with respect to the Fourier regression.\n",
    "\n",
    "No, it is not a state-of-the-art model.\n",
    "\n",
    "But it did the job. \n",
    "\n",
    "The MAPE on the COVID period is much lower than before, and you are satisfied with such results. You get a lot of questions about more complex time-series-focused model, but this will be topic for another day.\n",
    "\n",
    "In fact, you have wondered about the issue for quite a while. Would a more complex and appropriate model for time series be able to improve? You read on the AWS documentation that one of their built-in algorithms is tailored to time series. It is more suited to sets of series with similar structure, but maybe it can help also with this problem.\n",
    "\n",
    "Giving it a try looks just so easy..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "If you’re ready to be done with this notebook, please run the cells below with `CLEANUP = True`. \n",
    "\n",
    "This will remove the model, hosted endpoint, and all the experiments you created to avoid any charges from a stray instance being left on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANUP = True\n",
    "if CLEANUP:\n",
    "    dense_predictor.delete_model()\n",
    "    dense_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
