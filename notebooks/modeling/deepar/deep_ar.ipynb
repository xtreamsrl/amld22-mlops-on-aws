{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepAR\n",
    "\n",
    "Finally, you consider a more complex alternative to the dense neural network architecture.\n",
    "\n",
    "[DeepAR](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html) is a supervised learning forecasting algorithm using recurrent neural networks (RNN); developed by Salinas, Flunkert, Gasthaus from Amazon research, it can be used in Amazon SageMaker.\n",
    "\n",
    "The main feature which makes the algorithm stand out is the ability to learn a single global model using many related time series, and to apply the model on potentially unseen new series. Think about an energy provider training DeepAR on individual household electrical consumption time series, and then applying inference to predict future consumption of new customers (that have no history to feed the model).\n",
    "\n",
    "The way the algorithm is able to correlate different time series at different times is by accepting/creating categorical features that label either single time indices or whole series.\n",
    "In the electrical consumption case, for example, day of the week, week of the year or being a holiday are labels of the first type (pertaining a single time tick, across all time series), while domain knowledge labels such as number of people in the household or number of rooms of the house are attached of course to whole individual series.\n",
    "\n",
    "In practice, the model takes in a two-dimensional matrix of real-valued data, where rows represent time indices and columns different series, and on top of this it assigns a set of labels to each element of the matrix depending on its row and column.\n",
    "\n",
    "For technical details about DeepAR we refer to the introductory paper [DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks](https://arxiv.org/abs/1704.04110).\n",
    "\n",
    "Note that we won't use multiple time series nor the extra categorical features for our toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read data from S3\n",
    "! pip install pandas s3fs --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, restart the kernel if this is the first time you run this notebook.\n",
    "\n",
    "This is necessary to ensure that we can actually import the libraries we've just installed in the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, CategoricalParameter, ContinuousParameter\n",
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuring the default size for matplotlib plots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (20, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ml.m5.xlarge' is included in the AWS Free Tier\n",
    "INSTANCE_TYPE = 'ml.m5.xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data gathering\n",
    "\n",
    "You start, as before, with the final dataset that the processing pipeline deposits on S3.\n",
    "In order to retrieve it, some authentication and path-related variables need to be declared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_END = '2019-12-31 23:59'\n",
    "TRAIN_END = '2018-12-31 23:59'\n",
    "NOW = '2020-05-31'\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_bucket = sagemaker_session.default_bucket()\n",
    "main_prefix = \"amld22-workshop-sagemaker\"\n",
    "\n",
    "data_bucket = f\"s3://{sagemaker_bucket}/{main_prefix}/data\"\n",
    "modelling_data_bucket = f'{data_bucket}/modelling/deepar'\n",
    "\n",
    "raw_data_s3_path = \"s3://public-workshop/normalized_data/processed/2006_2022_data.parquet\"\n",
    "\n",
    "raw_df = pd.read_parquet(raw_data_s3_path)\n",
    "resampled_df = raw_df.resample('D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_series = resampled_df[:NOW].Load\n",
    "load_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you want to try hyperparameter tuning, you need to split the dataset into train, test and validation. As always, you place yourself at the beginning of 2020; The whole of 2019 will be the validation set.\n",
    "\n",
    "After tuning, you'll want to re-train the best model with all available data, including the validation set; for this reason let's define `train_ext` as the union of training and validation.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Simplification.</b> \n",
    "\n",
    "This is true for any model, including the Fourier regression and dense neural network. In real life, you always want to use all the data to train the production model (unless old data are not relevant anymore).\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_train = load_series[:TRAIN_END]\n",
    "load_validation = load_series[TRAIN_END:VALIDATION_END]\n",
    "load_train_ext = load_series[:VALIDATION_END]\n",
    "load_test = load_series[VALIDATION_END:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload on S3\n",
    "\n",
    "DeepAR needs all its time series to be parsed and stored as json files on S3. Each file contains information about the beginning and the actual values of the series; their periodicity will be provided as a model parameter.\n",
    "\n",
    "These files are first written locally, then copied to S3 before deleting the local version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data =[\n",
    "    {\n",
    "        \"start\": str(load_train.index[0]),\n",
    "        \"target\": load_train.tolist(),\n",
    "    }\n",
    "]\n",
    "validation_data =[\n",
    "    {\n",
    "        \"start\": str(load_validation.index[0]),\n",
    "        \"target\": load_validation.tolist(),\n",
    "    }\n",
    "]\n",
    "training_ext_data =[\n",
    "    {\n",
    "        \"start\": str(load_train_ext.index[0]),\n",
    "        \"target\": load_train_ext.tolist(),\n",
    "    }\n",
    "]\n",
    "test_data =[\n",
    "    {\n",
    "        \"start\": str(load_test.index[0]),\n",
    "        \"target\": load_test.tolist(),\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"validation.json\", validation_data)\n",
    "write_dicts_to_file(\"train_ext.json\", training_ext_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = boto3.resource(\"s3\").Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(f\"File {s3_path} already exists.\\nSet override to upload anyway.\\n\")\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_s3(\"train.json\", f\"{modelling_data_bucket}/train/train.json\")\n",
    "copy_to_s3(\"validation.json\", f\"{modelling_data_bucket}/validation/validation.json\")\n",
    "copy_to_s3(\"train_ext.json\", f\"{modelling_data_bucket}/train_ext/train_ext.json\")\n",
    "copy_to_s3(\"test.json\", f\"{modelling_data_bucket}/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "You are now ready to define the DeepAR estimator object, denoted as `forecasting-deepar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", sagemaker_session.boto_region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=1,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    max_run=60*30,\n",
    "    max_wait=60*45,\n",
    "    use_spot_instances=True # See https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "You look around the internet for a good set of hyperparameters to provide the model. You can refer to [this](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-tuning.html) for the main hyperparameters and tuning metrics; [here](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html) you can find an explanation of each parameter as well as their ranges and default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hyperparams = {\n",
    "    \"time_freq\": \"1D\",\n",
    "    \"epochs\": \"35\", # 335\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"1E-3\",\n",
    "    \"context_length\": \"7\",\n",
    "    \"prediction_length\": \"1\",\n",
    "    \"num_cells\": \"84\",\n",
    "    \"num_layers\":\"2\",\n",
    "    \"dropout_rate\": \"0.1\",\n",
    "    \"embedding_dimension\": \"10\",\n",
    "    \"likelihood\": \"student-T\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**default_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are not satisfied, though. \n",
    "\n",
    "You find out that [Amazon SageMaker automatic model tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) is a powerful tool for automatic hyperparameter tuning.\n",
    "\n",
    "You just define a `HyperparameterTuner` object, provide it with the estimator and ranges for the parameters you want to tune, and specify how many trials to perform in total (`max_jobs`) and contemporarily (`max_parallel_jobs`).\n",
    "\n",
    "Once you've run the job by calling its `fit` method, you can follow all trials in your SageMaker Dashboard.\n",
    "\n",
    "![SageMaker dashboard](img/sagemaker_dashboard_annotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading about the billing model of the service, you decide not to be too crazy and just optimize over one hyperparameter.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning.</b> \n",
    "\n",
    "You can optimize over a wider grid - and in real life you would. We are not doing it here to keep it short and avoid charges.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'epochs': IntegerParameter(200, 400),\n",
    "    #\"context_length\": IntegerParameter(5, 14),\n",
    "    #\"mini_batch_size\": IntegerParameter(64, 128),    \n",
    "    #'learning_rate': ContinuousParameter(0.0001, 0.01),\n",
    "    #\"num_cells\": IntegerParameter(60, 100),\n",
    "    #\"num_layers\": IntegerParameter(1, 8),\n",
    "    #\"dropout_rate\": ContinuousParameter(0, 0.2),\n",
    "    #\"embedding_dimension\": IntegerParameter(1, 20)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'test:RMSE'\n",
    "objective_metric_type = 'Minimize'\n",
    "metric_definitions = [{\n",
    "    'Name': 'test:RMSE',\n",
    "    'Regex': 'test:RMSE=([0-9\\\\.]+)'\n",
    "}]\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=objective_metric_name,    \n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    objective_type=objective_metric_type,\n",
    "    max_jobs=4,\n",
    "    max_parallel_jobs=4\n",
    ")\n",
    "#tuner.fit({\n",
    "#    \"train\": f\"{modelling_data_bucket}/train\", \n",
    "#    \"test\": f\"{modelling_data_bucket}/validation\"\n",
    "#})\n",
    "#job_name = tuner._current_job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning has completed, you can inspect the effect of parameter values with respect to the chosen metric by help of the `HyperparameterTuningJobAnalytics` object and some code for nice plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuner_analytics = sagemaker.HyperparameterTuningJobAnalytics(job_name)\n",
    "#full_df = tuner_analytics.dataframe().sort_values('FinalObjectiveValue')\n",
    "#full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(full_df.sort_values(\"epochs\")['epochs'], full_df.sort_values(\"epochs\")[\"FinalObjectiveValue\"])\n",
    "#plt.title('Loss vs Epochs')\n",
    "#plt.xlabel('epochs')\n",
    "#plt.ylabel('loss')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when you have found a satisfactory set of parameters, you can select it directly from the analytics output and plug it back into the estimator for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_hyperparams = dict(full_df[full_df.columns.intersection(list(default_hyperparams.keys()))].iloc[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_hyperparams = dict(default_hyperparams, **best_hyperparams)\n",
    "#for k, v in final_hyperparams.items():  # for some reason tuning turns int params into float\n",
    "#    if not isinstance(v, str):\n",
    "#        if v == int(v):\n",
    "#            final_hyperparams[k] = str(int(v))\n",
    "#        else:\n",
    "#            final_hyperparams[k] = str(v)\n",
    "#final_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#estimator.set_hyperparameters(**final_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "As mentioned before, you're going to want to use all available data for training, and this means using the `train_ext` dataset defined at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": f\"{modelling_data_bucket}/train_ext\", \n",
    "        \"test\": f\"{modelling_data_bucket}/validation\"\n",
    "    },\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "As you already did for previous models, you deploy your DeepAR instance on a managed endpoint as a `DeepARPredictor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, \n",
    "    instance_type=INSTANCE_TYPE, \n",
    "    predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "Rolling, 1 day ahead prediction inferences on the test set; each time you can pass the whole time series up to the day before the target, since DeepAR uses all information it can, even without re-training every day.\n",
    "\n",
    "You look at the MAPE performance on COVID period only, since that's the short-term horizon you are interested into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_preds = [predictor.predict(ts=load_series[:i], quantiles=[0.5]) for i in load_series[load_train_ext.index[-1]:].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_series = pd.concat([p.loc[:, '0.5'] for p in rolling_preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred) / y_pred)\n",
    "\n",
    "prediction_df = pd.DataFrame({\"actual\": load_test, \"predicted\": prediction_series}).dropna()\n",
    "mape = mean_absolute_percentage_error(prediction_df[\"actual\"], prediction_df[\"predicted\"])\n",
    "\n",
    "plt.title(f\"DeepAR | Covid MAPE: {mape:.2%}\")\n",
    "plt.plot(prediction_df[\"actual\"], label='actual')\n",
    "plt.plot(prediction_df[\"predicted\"], label='predicted')\n",
    "plt.legend()\n",
    "plt.grid(0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_mape_df = pd.DataFrame(\n",
    "    {'rolling_mape': map(lambda w: mean_absolute_percentage_error(w[\"actual\"], w[\"predicted\"]),\n",
    "                         prediction_df.rolling(7))},\n",
    "    index=prediction_df.index)\n",
    "plt.plot(rolling_mape_df.rolling_mape)\n",
    "plt.title('Rolling MAPE (7-day window)')\n",
    "plt.grid(0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "If you’re ready to be done with this notebook, please run the cells below with `CLEANUP = True`.\n",
    "\n",
    "This will remove the model and the hosted endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANUP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEANUP:\n",
    "    predictor.delete_model()\n",
    "    predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
