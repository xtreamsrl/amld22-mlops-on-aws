{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data ingestion pipeline\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Objective</b> \n",
    "    \n",
    "In this notebook, we will show how you can automate the data ingestion process. The pipeline will take raw data, parse it, transform it, validate it and then merge it before storing it into an S3 bucket.\n",
    "</div>\n",
    "\n",
    "We will be using a vanilla Process with a custom Docker image that we have pre-built for you.\n",
    "\n",
    "(You can always built is yourself, but not here: Sagemaker studio does not have Docker so you would have to build it on your own machine or on a Sagemaker Notebook Instance)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Data integrity</b> \n",
    "    \n",
    "We will make use of **deepchecks** to validate the data and to mark the pipeline as successful.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: stepfunctions in /opt/conda/lib/python3.7/site-packages (2.3.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from stepfunctions) (6.0)\n",
      "Requirement already satisfied: boto3>=1.14.38 in /opt/conda/lib/python3.7/site-packages (from stepfunctions) (1.20.23)\n",
      "Requirement already satisfied: sagemaker>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from stepfunctions) (2.70.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.14.38->stepfunctions) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.14.38->stepfunctions) (0.5.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.14.38->stepfunctions) (1.23.24)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (1.5.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (3.19.1)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (0.2.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (0.1.5)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /root/.local/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/.local/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (21.3)\n",
      "Requirement already satisfied: pandas in /root/.local/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (1.3.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (0.2.8)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker>=2.1.0->stepfunctions) (19.3.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.14.38->stepfunctions) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.14.38->stepfunctions) (1.26.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker>=2.1.0->stepfunctions) (2.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker>=2.1.0->stepfunctions) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker>=2.1.0->stepfunctions) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker>=2.1.0->stepfunctions) (2019.3)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.1.0->stepfunctions) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.1.0->stepfunctions) (0.3.4)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.1.0->stepfunctions) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker>=2.1.0->stepfunctions) (0.70.12.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTANCE_TYPE = 'ml.m5.xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "import sagemaker\n",
    "import stepfunctions\n",
    "import json\n",
    "from sagemaker import Processor\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_output_path = \"data\"\n",
    "main_path = \"amld22-workshop-sagemaker\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: eu-west-1 | Default bucket: sagemaker-eu-west-1-919788038405 | Role: arn:aws:iam::919788038405:role/service-role/AmazonSageMaker-ExecutionRole-20210604T120344\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sessions = sagemaker.Session()\n",
    "account = boto_session.client('sts').get_caller_identity()['Account']\n",
    "default_bucket = sessions.default_bucket()\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "print(f\"Region: {region} | Default bucket: {default_bucket} | Role: {role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we upload all the raw data (available locally) to the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded: amld22-workshop-sagemaker/data/raw/ingestion/faulty_ingestion_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/ingestion/good_ingestion_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/entsoe-2016/2016_2017_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/terna/2017_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/terna/2022_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/terna/2019_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/terna/2020_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/terna/2018_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/terna/2021_data.xlsx\n",
      "File uploaded: amld22-workshop-sagemaker/data/raw/entsoe-2006/2006_2015_data.xlsx\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "for file in Path('raw').glob('**/*.xlsx'):\n",
    "    s3.upload_file(\n",
    "        f'{file}',\n",
    "        default_bucket,\n",
    "        f'{main_path}/{data_output_path}/{file}'\n",
    "    )\n",
    "    print(f\"File uploaded: {main_path}/{data_output_path}/{file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_input_data_path = f's3://{default_bucket}/{main_path}/{data_output_path}/raw'\n",
    "output_data_path = f's3://{default_bucket}/{main_path}/{data_output_path}/normalized_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to build your docker image. To start, you create the ECR (Elastic Container Registry) repository. Then, you build the Docker image with the code to process the dataset and push it to such dataset.    \n",
    "\n",
    "Also this time, your favourite ML Engineers have already done it for you, and you can use it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#import subprocess\n",
    "#docker_build = subprocess.check_output(f'bash ./build_push.sh prepare_data eu-west-1', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_uri = \"919788038405.dkr.ecr.eu-west-1.amazonaws.com/prepare_data:latest\"\n",
    "\n",
    "processor = Processor(\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  858af98a01f94abba83492e0d455a472-docker-data-processor\n",
      "Inputs:  [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-west-1-919788038405/amld22-workshop-sagemaker/data/raw', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'normalized_data', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-west-1-919788038405/amld22-workshop-sagemaker/data/normalized_data', 'LocalPath': '/opt/ml/processing/normalized_data', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".......................\u001b[34mpandas version: 1.4.1\u001b[0m\n",
      "\u001b[34m{'ProcessingJobArn': 'arn:aws:sagemaker:eu-west-1:919788038405:processing-job/858af98a01f94abba83492e0d455a472-docker-data-processor', 'ProcessingJobName': '858af98a01f94abba83492e0d455a472-docker-data-processor', 'AppSpecification': {'ImageUri': '919788038405.dkr.ecr.eu-west-1.amazonaws.com/prepare_data:latest', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input', 'S3Uri': 's3://sagemaker-eu-west-1-919788038405/amld22-workshop-sagemaker/data/raw', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinition': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'normalized_data', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/normalized_data', 'S3Uri': 's3://sagemaker-eu-west-1-919788038405/amld22-workshop-sagemaker/data/normalized_data', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 30, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::919788038405:role/service-role/AmazonSageMaker-ExecutionRole-20210604T120344', 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}}\u001b[0m\n",
      "\u001b[34mout path: /opt/ml/processing/normalized_data\u001b[0m\n",
      "\u001b[34minput path: /opt/ml/processing/input\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:27,994 - root - INFO - Processing entsoe-2006...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:29,290 - root - INFO - After melting, found 10 NaNs. Interpolating...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:29,305 - root - INFO - Processed entsoe-2006. Saving...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:29,333 - root - INFO - Saved entsoe-2006 to /opt/ml/processing/normalized_data/interim/2006_2015_data.parquet.\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:29,333 - root - INFO - Processing entsoe-2016...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:31,365 - root - INFO - Converting timezones...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:31,383 - root - INFO - After conversion, dataframe has shape (8784, 1)\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:31,387 - root - INFO - After re-indexing and interpolation, dataframe has shape (8784, 1)\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:31,387 - root - INFO - Processed entsoe-2016. Saving...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:31,390 - root - INFO - Saved entsoe-2016 to /opt/ml/processing/normalized_data/interim/2016_data.parquet.\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:31,390 - root - INFO - Processing Terna files...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:31,391 - root - INFO - 6 raw input data files found: ['2022_data.xlsx', '2020_data.xlsx', '2018_data.xlsx', '2017_data.xlsx', '2019_data.xlsx', '2021_data.xlsx']\u001b[0m\n",
      "\u001b[34m2022-03-25 23:14:31,391 - root - INFO - Reading terna input data files...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:15:12,771 - root - INFO - Resampling Terna data...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:15:12,889 - root - INFO - After resampling, data has shape (44736, 1)\u001b[0m\n",
      "\u001b[34m2022-03-25 23:15:12,889 - root - INFO - Processed terna. Saving...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:15:12,909 - root - INFO - Saved terna to /opt/ml/processing/normalized_data/interim/2017_2022_data.parquet.\u001b[0m\n",
      "\u001b[34m2022-03-25 23:15:12,910 - root - INFO - Concatenating data...\u001b[0m\n",
      "\u001b[34m2022-03-25 23:15:12,910 - root - INFO - Found 3 interim files: ['2017_2022_data.parquet', '2006_2015_data.parquet', '2016_data.parquet']\u001b[0m\n",
      "\u001b[34m2022-03-25 23:15:12,947 - root - INFO - Saving processed dataset\u001b[0m\n",
      "\u001b[34m2022-03-25 23:15:12,990 - root - INFO - Saved processed dataset to /opt/ml/processing/normalized_data/processed/2006_2022_data.parquet.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:108: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.hide(axis='index')`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:108: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.hide(axis='index')`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:108: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.hide(axis='index')`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:108: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.hide(axis='index')`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_suite.py:164: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.hide(axis='index')`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:108: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.hide(axis='index')`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_suite.py:164: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.hide(axis='index')`\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.9/site-packages/deepchecks/base/display_pandas.py:54: FutureWarning:\u001b[0m\n",
      "\u001b[34mthis method is deprecated in favour of `Styler.to_html()`\u001b[0m\n",
      "\u001b[34m#015Single Dataset Integrity Suite:   0%|        | 0/8 [00:00<?, ? Check/s]#015Single Dataset Integrity Suite:   0%|        | 0/8 [00:00<?, ? Check/s, Check=Is Single Value]#015Single Dataset Integrity Suite:  12%|█       | 1/8 [00:00<00:00, 770.73 Check/s, Check=Mixed Nulls]#015Single Dataset Integrity Suite:  25%|██      | 2/8 [00:00<00:00, 389.52 Check/s, Check=Mixed Data Types]#015Single Dataset Integrity Suite:  38%|███     | 3/8 [00:00<00:00, 356.95 Check/s, Check=String Mismatch] #015Single Dataset Integrity Suite:  50%|████    | 4/8 [00:00<00:00, 410.31 Check/s, Check=Data Duplicates]#015Single Dataset Integrity Suite:  62%|█████   | 5/8 [00:00<00:00, 260.28 Check/s, Check=String Length Out Of Bounds]#015Single Dataset Integrity Suite:  75%|██████  | 6/8 [00:00<00:00, 147.13 Check/s, Check=Special Characters]         #015Single Dataset Integrity Suite:  88%|███████ | 7/8 [00:00<00:00, 160.94 Check/s, Check=Label Ambiguity]   #015                                                                                                       #015#015Single Dataset Integrity Suite:   0%|        | 0/8 [00:00<?, ? Check/s]#015Single Dataset Integrity Suite:   0%|        | 0/8 [00:00<?, ? Check/s, Check=Is Single Value]#015Single Dataset Integrity Suite:  12%|█       | 1/8 [00:00<00:00, 879.86 Check/s, Check=Mixed Nulls]#015Single Dataset Integrity Suite:  25%|██      | 2/8 [00:00<00:00, 1047.01 Check/s, Check=Mixed Data Types]#015Single Dataset Integrity Suite:  38%|███     | 3/8 [00:00<00:00, 910.49 Check/s, Check=String Mismatch]  #015Single Dataset Integrity Suite:  50%|████    | 4/8 [00:00<00:00, 1097.55 Check/s, Check=Data Duplicates]#015Single Dataset Integrity Suite:  62%|█████   | 5/8 [00:00<00:00, 821.48 Check/s, Check=String Length Out Of Bounds]#015Single Dataset Integrity Suite:  75%|██████  | 6/8 [00:00<00:00, 402.20 Check/s, Check=Special Characters]         #015Single Dataset Integrity Suite:  88%|███████ | 7/8 [00:00<00:00, 406.48 Check/s, Check=Label Ambiguity]   #015                                                                                                       #015\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=raw_input_data_path,\n",
    "            destination=\"/opt/ml/processing/input\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"normalized_data\",\n",
    "            destination=output_data_path,\n",
    "            source=\"/opt/ml/processing/normalized_data\"\n",
    "        )\n",
    "    ],\n",
    "    job_name=f\"docker-data-processor-{uuid.uuid4().hex}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the deepchecks have passed, so the data has been processed and ready to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing, we need to add some permissions to the role we're using to run this notebook. \n",
    "Please, add the following Trust Relationship services:     \n",
    "\n",
    "`\n",
    "[\n",
    "    \"sagemaker.amazonaws.com\",\n",
    "    \"events.amazonaws.com\",\n",
    "    \"lambda.amazonaws.com\",\n",
    "    \"states.amazonaws.com\"\n",
    "]\n",
    "`    \n",
    "\n",
    "Also, just to be sure, add the AdministratorAccess policy to the same role (**NEVER EVER DO THIS IN PRODUCTION**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now create some data that will make the pipeline fail. This will show the usefulness of the data checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we set up the recipient fo the emails in case of a failure in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns = boto3.resource('sns')\n",
    "\n",
    "try:\n",
    "    for topic in sns.topics.all():\n",
    "        if topic.arn.split(':')[-1] == 'failure-topic':\n",
    "            topic.delete()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "sns_topic_arn = sns.meta.client.create_topic(Name='failure-topic')['TopicArn']\n",
    "\n",
    "topic = sns.Topic(sns_topic_arn)\n",
    "\n",
    "for email in [\n",
    "    'gabriele.mazzola@xtreamers.io', \n",
    "    'emanuele.fabbiani@xtreamers.io', \n",
    "    'gabriele.orlandi@xtreamers.io', \n",
    "    'matteo.moroni@besharp.it'\n",
    "]:\n",
    "    topic.subscribe(\n",
    "        Protocol='email',\n",
    "        Endpoint=email\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Before continuing** make sure that the recipient accept the subscription so people can receive the notification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then, we create a step function which is triggered by an S3 upload."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "https://docs.aws.amazon.com/step-functions/latest/dg/tutorial-cloudwatch-events-s3.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stepfunctions.inputs import Placeholder\n",
    "from stepfunctions.workflow import Workflow\n",
    "from stepfunctions.steps import ProcessingStep, Catch, Fail, Task\n",
    "# generate step function\n",
    "from stepfunctions.steps import Succeed, SnsPublishStep\n",
    "\n",
    "step_function_name = 'data_ingestion_step_function'\n",
    "\n",
    "end_stepfunction = Succeed(\n",
    "            state_id=\"Success\"\n",
    ")\n",
    "\n",
    "fail_state = Fail(\n",
    "    state_id=\"Fail\",\n",
    ")\n",
    "\n",
    "end_stepfunction.update_parameters({\"end\": True})\n",
    "\n",
    "error_step = SnsPublishStep(\n",
    "    state_id=\"Fail state message\",\n",
    "    parameters={\n",
    "        \"TopicArn\": sns_topic_arn,\n",
    "        \"Message.$\": \"$\",\n",
    "    }\n",
    ")\n",
    "error_step.next(fail_state)\n",
    "\n",
    "\n",
    "ingest_data_step = Task(\n",
    "    state_id='Ingest data step',\n",
    "    resource=\"arn:aws:states:::sagemaker:createProcessingJob.sync\",\n",
    "    parameters={\n",
    "        \"ProcessingJobName.$\": \"$$.Execution.Input.id\",\n",
    "        \"AppSpecification\": {\n",
    "            \"ImageUri\": image_uri,\n",
    "        },\n",
    "        \"RoleArn\": role,\n",
    "        \"ProcessingInputs\": [\n",
    "                ProcessingInput(\n",
    "                    input_name=\"data\",\n",
    "                    source=raw_input_data_path,\n",
    "                    destination=\"/opt/ml/processing/input\"\n",
    "                )._to_request_dict()\n",
    "            ],\n",
    "            \"ProcessingOutputConfig\": {\n",
    "                \"Outputs\": [\n",
    "                    ProcessingOutput(\n",
    "                        output_name=\"normalized_data\",\n",
    "                        destination=output_data_path,\n",
    "                        source=\"/opt/ml/processing/normalized_data\"\n",
    "                    )._to_request_dict()\n",
    "                ]\n",
    "            },\n",
    "        \"ProcessingResources\": {\n",
    "            \"ClusterConfig\": {\n",
    "                \"InstanceCount\": 1,\n",
    "                \"InstanceType\": INSTANCE_TYPE,\n",
    "                \"VolumeSizeInGB\": 30\n",
    "            },\n",
    "\n",
    "        },\n",
    "        \"StoppingCondition\": {\n",
    "            \"MaxRuntimeInSeconds\": 600\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "ingest_data_step.add_catch(Catch(\n",
    "    error_equals=[\"States.TaskFailed\"],\n",
    "    next_step=error_step\n",
    "))\n",
    "\n",
    "ingest_data_step.next(end_stepfunction)\n",
    "\n",
    "workflow = Workflow(\n",
    "    name=step_function_name,\n",
    "    definition=ingest_data_step,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stepfunctions.exceptions import WorkflowNotFound\n",
    "\n",
    "# workflow.delete()\n",
    "sfn = boto3.client('stepfunctions')\n",
    "workflow_arn = f'arn:aws:states:{region}:{account}:stateMachine:{step_function_name}'\n",
    "try:\n",
    "    sfn.update_state_machine(\n",
    "        stateMachineArn=workflow_arn,\n",
    "        definition=json.dumps(workflow.definition.to_dict())\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    workflow_arn = workflow.create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'V48RC5CJQ5KJJ0FN',\n",
       "  'HostId': 'IcbMGw1dW0r1q90HXDDO5FvGwe5yF3EWV5ZJoPuFYvJ1rc9Qj99p5GSwkbvU86GyiWfsVb8ZMLA=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'IcbMGw1dW0r1q90HXDDO5FvGwe5yF3EWV5ZJoPuFYvJ1rc9Qj99p5GSwkbvU86GyiWfsVb8ZMLA=',\n",
       "   'x-amz-request-id': 'V48RC5CJQ5KJJ0FN',\n",
       "   'date': 'Sat, 26 Mar 2022 00:14:25 GMT',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "\n",
    "s3.meta.client.put_bucket_notification_configuration(\n",
    "    Bucket=default_bucket,\n",
    "    NotificationConfiguration={\n",
    "        'EventBridgeConfiguration': {}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step function deployed to trigger on upload at: amld22-workshop-sagemaker/data/raw\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "events = boto3.client('events')\n",
    "events.put_rule(\n",
    "        Name='amld22-data-ingestion-rule',\n",
    "        EventPattern=json.dumps({\n",
    "        \"source\": [\n",
    "            \"aws.s3\"\n",
    "        ],\n",
    "        \"detail-type\": [\n",
    "            \"Object Created\"\n",
    "        ],\n",
    "        \"detail\": {\n",
    "            \"bucket\": {\n",
    "                \"name\": [\n",
    "                    default_bucket\n",
    "                ]\n",
    "            },\n",
    "            \"object\": {\n",
    "                \"key\": [\n",
    "                    {\n",
    "                        \"prefix\": f\"{main_path}/{data_output_path}/raw\",\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }),\n",
    "    State='ENABLED',\n",
    "    Description='Start ingestion step function when data is uploaded in s3',\n",
    "    RoleArn=role\n",
    ")\n",
    "\n",
    "print(f\"Step function deployed to trigger on upload at: {main_path}/{data_output_path}/raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FailedEntryCount': 0,\n",
       " 'FailedEntries': [],\n",
       " 'ResponseMetadata': {'RequestId': 'ce611e19-0a58-4ed2-914e-3bdf06977206',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ce611e19-0a58-4ed2-914e-3bdf06977206',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '41',\n",
       "   'date': 'Sat, 26 Mar 2022 00:14:28 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets = events.list_targets_by_rule(Rule='amld22-data-ingestion-rule')['Targets']\n",
    "if targets:\n",
    "    print(\"Rule already has some targets. Let's clean it up.\")\n",
    "    ids_to_remove = [t['Id'] for t in targets]\n",
    "    events.remove_targets(Rule='amld22-data-ingestion-rule', Ids=ids_to_remove)\n",
    "    \n",
    "events.put_targets(\n",
    "    Rule='amld22-data-ingestion-rule',\n",
    "    Targets=[\n",
    "        {\n",
    "            'Id': uuid.uuid4().hex,\n",
    "            'Arn': workflow_arn,\n",
    "            'RoleArn': role\n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-eu-west-1-919788038405'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_bucket"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
